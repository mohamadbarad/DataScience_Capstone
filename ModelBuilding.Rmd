---
title: "Data Loading and Exploratory Analysis "
author: "Mohamad Barad"
date: "19/2/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

## Introduction

This exercise is about working with our data and do some exploratory analysis of the data in order to prepare for building prediction models in the upcoming exercises and the shiny app development. First the data is loaded and then stored in a rds data set in order to clean up the environment and save some memory. Then data is then pre-processed and then visualised during the exploratory analysis. 
Since the processes in this project takes much of the memory which decreases the efficiency of the code, only part of the data will be used. This is done by 'sampling' the data and only use part of it. In this exercise 25 % of the each og the three data sets was used. 
The buliding of N-gram models will be done using 'tidytext' package, which is convinent other packages are also an option, however memory usage should definitely be taken into consideration. 

```{r packages}
# Loading Packages 
library(tm)
library(stringi)
library(SnowballC)
library(tidyverse)
library(tidytext)
library(wordcloud)
# library(RWeka)
library(ggplot2)
library(quanteda)
library(data.table)

# library(readr)

#Setting the seed
set.seed(3434)
```

# Data loading 
Since on of the limitations of this exercise is memory, only part of the data will be used. This is done by reading the whole data from the three datasets 'blog', 'news' and 'Twitter', and then sampling 25 % of the data for analysis.  

```{r loading data}
# Blog text file
con <- file("00ProjectData/Training/final/en_US/en_US.blogs.txt",open = "rb",encoding = "ACSII")
linesInFile.blog <- readLines(con,encoding="UTF-8",skipNul = TRUE)
fileSize.blog <- format(object.size(linesInFile.blog),units = "MB")
fileNoOfLines.blog <- length(linesInFile.blog)
fileWords.blog <- sum(stri_count_words(linesInFile.blog))
close(con)
info.blog <- paste0("File size: ", fileSize.blog, " Lines in File: ", fileNoOfLines.blog, " Words in file: ", fileWords.blog)

# News text file
con <- file("00ProjectData/Training/final/en_US/en_US.news.txt.",open = "rb",encoding = "ACSII")
linesInFile.news <- readLines(con,encoding="UTF-8",skipNul = TRUE)
fileSize.news <- format(object.size(linesInFile.news),units = "MB")
fileNoOfLines.news <- length(linesInFile.news)
fileWords.news <- sum(stri_count_words(linesInFile.news))
close(con)
info.news <- paste0("File size: ", fileSize.news, " Lines in File: ", fileNoOfLines.news, " Words in file: ", fileWords.news)


# Twitter text file
con <- file("00ProjectData/Training/final/en_US/en_US.twitter.txt", open = "rb",encoding = "ACSII")
linesInFile.twitter <- readLines(con,encoding="UTF-8",skipNul = TRUE)
fileSize.twitter <- format(object.size(linesInFile.twitter),units = "Mb")
fileNoOfLines.twitter <- length(linesInFile.twitter)
fileWords.twitter <- sum(stri_count_words(linesInFile.twitter))
close(con)
info.twitter <- paste0("File size: ", fileSize.twitter, " Lines in File: ", fileNoOfLines.twitter, " Words in file: ", fileWords.twitter)


# Sampling from data
datalist <- list(blogData = linesInFile.blog,
                 newsData = linesInFile.news,
                 twitterData = linesInFile.twitter)



# Summary of data
DataSummary <- data.frame(Document = c("blog","news","twitter"), 
                          size = c(fileSize.blog,fileSize.news,fileSize.twitter),
                          NumberOfLines = c(fileNoOfLines.blog,fileNoOfLines.news,fileNoOfLines.twitter),
                          NumberOfWords = c(fileWords.blog,fileWords.news,fileWords.twitter)
)
DataSummary

# Saving data samples in RDS
saveRDS(datalist, "datalist.rds")

# Cleaning the environment 
rm(list=ls())
gc()
```

# Preprocessing 
After cleaning up the environ ment and the dataset samples is stored, the data is then loaded and prepared for the pre-processing which include:
\item Punctuation replacement
\item Converting to lower case
\item Removing numbers
\item Removing stopwords
\item Removing extra white spaces
\item Word-stemming 
\item Converting the document to plain text 

```{r pre processing, eval}
# Reading data
datalist <- readRDS("datalist.rds")

dtmToDf <- function(DocTermMatrix){
  freq <- colSums(DocTermMatrix)
  terms <- data.frame(term = names(freq), frequency = freq, stringsAsFactors = FALSE)
  terms <- arrange(terms, desc(frequency))
  rownames(terms) <- 1:length(freq)
  terms <- terms[!(is.na(terms$term) | terms$term==""), ]
  return(terms)
}

# Clean and Tokenize into sentences
system.time(TokSent <- lapply(datalist,function(x){
  tmp <- quanteda::corpus(x)
  corpus_reshape(tmp,to = "sentences")
})) 

system.time(unigram <- lapply(TokSent, function(x){
  t<- tokens(x,
             what = "word",
             remove_punct = TRUE,
             remove_symbols = TRUE,
             remove_numbers = TRUE,
             remove_separators = TRUE,
             split_hyphens = TRUE,
             padding = TRUE,
             verbose = TRUE)
  tokens_select(t,pattern=stopwords("en"), selection = "remove")
}))
rm(TokSent)

system.time(bigram <- lapply(unigram, function(x){
  tokens_ngrams(x,n = 2) # Bigram
}))

system.time(trigram <- lapply(unigram, function(x){
  tokens_ngrams(x,n = 3) # Trigram
}))
system.time(tetragram <- lapply(unigram, function(x){
  tokens_ngrams(x,n = 4) # Tetragram
}))

system.time(pentagram <- lapply(unigram, function(x){
  tokens_ngrams(x,n = 5) # pentagram
}))

# system.time(hexagram <- lapply(unigram, function(x){
#   tokens_ngrams(x,n = 6)
# }))
```


```{r Document Feature and Visualisation}

### UNIGRAM ### START 
unigram <- readRDS("unigram.rds")
system.time(
  for (i in 1:3) {
    UniDocFeat <- dfm(unigram[[i]])     
})
UniDataFrame <- dtmToDf(UniDocFeat)
# Word cloud
wordcloud(word = UniDataFrame$term, freq = UniDataFrame$frequency, min.freq = 1,max.words = 50, 
          random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
# Bar plot:
p1 <- ggplot( UniDataFrame[1:20,]) + 
  geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top 20 unigrams",
       subtitle = "using quanteda in R",
       caption = "Data Source:")
p1
# saveRDS(UniDocFeat,"UniDocFeat.rds")
# saveRDS(UniDataFrame,"UniDataFrame.rds")
# rm(list=ls())
# gc()
### UNIGRAM ### END


### BIGRAM ### START 
bigram <- readRDS("bigram.rds")
system.time(
  for (i in 1:3) {
    BiDocFeat <- dfm(bigram[[i]])     
  }
)

BiDataFrame <- dtmToDf(BiDocFeat)

# Word cloud
wordcloud(word = BiDataFrame$term, freq = BiDataFrame$frequency, min.freq = 1,max.words = 50,random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))

# Bar plot:
p1 <- ggplot( BiDataFrame[1:20,]) + 
  geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top 20 unigrams",
       subtitle = "using quanteda in R",
       caption = "Data Source:")
p1
# saveRDS(BiDocFeat,"BiDocFeat.rds")
# saveRDS(BiDataFrame,"BiDataFrame.rds")
# rm(list=ls())
# gc()
### BIGRAM ### END


### TRIGRAM  ### START 
trigram <- readRDS("trigram.rds")
system.time(
  for (i in 1:3) {
    TriDocFeat <- dfm(trigram[[i]])     
  }
)
TriDataFrame <- dtmToDf(TriDocFeat)

# Word cloud
wordcloud(word = TriDataFrame$term, freq = TriDataFrame$frequency, min.freq = 1,max.words = 50,random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))

# Bar plot:
p1 <- ggplot( TriDataFrame[1:20,]) + 
  geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top 20 Trigrams",
       subtitle = "using quanteda in R",
       caption = "Data Source:")
p1
saveRDS(TriDocFeat,"TriDocFeat.rds")
saveRDS(TriDataFrame,"TriDataFrame.rds")
rm(list=ls())
gc()
### TRIGRAM  ### END

### TETRAGRAM ### START 
tetragram <- readRDS("tetragram.rds")
system.time(
  for (i in 1:3) {
    TetraDocFeat <- dfm(tetragram[[i]])     
  }
)

TetraDataFrame <- dtmToDf(TetraDocFeat)

# Word cloud
wordcloud(word = TetraDataFrame$term, freq = TetraDataFrame$frequency, min.freq = 1,max.words = 50, 
          random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))

# Bar plot:
p1 <- ggplot( TetraDataFrame[1:20,]) + 
  geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top 20 Tatragrams",
       subtitle = "using quanteda in R",
       caption = "Data Source:")
p1
saveRDS(TetraDocFeat,"TetraDocFeat.rds")
saveRDS(TetraDataFrame,"TetraDataFrame.rds")
rm(list=ls())
gc()
### TETRAGRAM ### END

### PENTAGRAM ### START 
pentagram <- readRDS("pentagram.rds")
system.time(
  for (i in 1:3) {
    PentaDocFeat <- dfm(Pentagram[[i]])     
})
PentaDataFrame <- dtmToDf(PentaDocFeat)
# Word cloud
wordcloud(word = PentaDataFrame$term, freq = PentaDataFrame$frequency, min.freq = 1,max.words = 50, 
          random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
# Bar plot:
p1 <- ggplot( PentaDataFrame[1:20,]) + 
  geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top 20 unigrams",
       subtitle = "using quanteda in R",
       caption = "Data Source:")
p1
saveRDS(PentaDocFeat,"PentaDocFeat.rds")
saveRDS(PentaDataFrame,"PentaDataFrame.rds")
rm(list=ls())
gc()
### UNIGRAM ### END

```


# Predictive model development 

```{r Conversion to DataTables}
OneGramDF <- readRDS("UniDataFrame.rds")
BiGramDF <- readRDS("BiDataFrame.rds")
TriGramDF <- readRDS("TriDataFrame.rds")

colnames(OneGramDF)[1] <- "LastTerm"
UniGramTable <- data.table(OneGramDF)

BiGramDF1 <- BiGramDF %>%
  separate(term, c("FirstTerms","LastTerm"),sep = "_") 
BiGramTable <- data.table(BiGramDF1)

TriGramDF1 <- TriGramDF %>% 
  separate(term, c("FirstTerms","MidTerm","LastTerm"),sep = "_") %>% 
  unite("FirstTerms",FirstTerms,MidTerm, sep = "_") 
TriGramTable <- data.table(TriGramDF1)


```

```{r N-grams with discounts}
# Source: https://github.com/ThachNgocTran/KatzBackOffModelImplementationInR/blob/master/cal
source("LeftOverProb.R")
source("GramTableExtended.R")
UniGramTable <- GramTableExtended(UniGramTable)

BiGramTable <- GramTableExtended(BiGramTable)
BiGramTable_leftOverProb = BiGramTable[, .(leftoverprob=LeftOverProb(LastTerm, frequency, discount)), by=FirstTerms]

TriGramTable <- GramTableExtended(TriGramTable)
TriGramTable <- TriGramTable[!(is.na(TriGramTable$LastTerm) | TriGramTable$LastTerm==""),]
TriGramTable_leftOverProb = TriGramTable[, .(leftoverprob=LeftOverProb(LastTerm, frequency, discount)), by=FirstTerms]

```

```{r Testing}
library(stringr)

source("SeparateTerms.R")
source("getLastTerms.R")
source("getTriGramProb.R")

p1 <- ggplot( TriGramDF[1:20,]) + 
  geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top 20 unigrams",
       subtitle = "using Tidytext in R",
       caption = "Data Source:")
p1

getTriGramProb("see arctic monkeys")

suggest3Gram <- TriGramTable_leftOverProb[which(TriGramTable_leftOverProb$leftoverprob>=probFinal),]
suggest3Gram[1:20,]

suggest3Gram <- TriGramTable[which(TriGramTable$prob>=probFinal),]
suggest3Gram[1:20,"LastTerm"]

suggest2Gram <- BiGramTable_leftOverProb[which(BiGramTable_leftOverProb$leftoverprob>=probFinal),]
suggest2Gram[1:20,]

suggest1Gram <- UniGramTable[which(UniGramTable$discount>=probFinal),]
suggest1Gram[1:20,]
```

```{r Next word}
colnames(BiGramTable)[1:2] <- c("word1","word2")

TriGramTable <- TriGramTable %>% 
  separate(FirstTerms, c("word1","word2"),sep = " ")
colnames(TriGramTable)[3] <- "word3"

source("PredictionFunctions.R")
predictNextWord("be insensitive")
```


