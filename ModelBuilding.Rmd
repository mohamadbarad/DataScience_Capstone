---
title: "Data Loading and Exploratory Analysis "
author: "Mohamad Barad"
date: "19/2/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
```

## Introduction

This exercise is about working with our data and do some exploratory analysis of the data in order to prepare for building prediction models in the upcoming exercises and the shiny app development. First the data is loaded and then stored in a rds data set in order to clean up the environment and save some memory. Then data is then pre-processed and then visualised during the exploratory analysis. 
Since the processes in this project takes much of the memory which decreases the efficiency of the code, only part of the data will be used. This is done by 'sampling' the data and only use part of it. In this exercise 25 % of the each og the three data sets was used. 
The buliding of N-gram models will be done using 'tidytext' package, which is convinent other packages are also an option, however memory usage should definitely be taken into consideration. 

```{r packages}
# Loading Packages 
library(tm)
library(stringi)
library(SnowballC)
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RWeka)
library(ggplot2)
library(data.table)

#Setting the seed
set.seed(3434)
```

# Data loading 
Since on of the limitations of this exercise is memory, only part of the data will be used. This is done by reading the whole data from the three datasets 'blog', 'news' and 'Twitter', and then sampling 25 % of the data for analysis.  

```{r loading data}
# Blog text file
con <- file("00ProjectData/Training/final/en_US/en_US.blogs.txt")
linesInFile.blog <- readLines(con,encoding="UTF-8",skipNul = TRUE)
fileSize.blog <- format(object.size(linesInFile.blog),units = "MB")
fileNoOfLines.blog <- length(linesInFile.blog)
fileWords.blog <- sum(stri_count_words(linesInFile.blog))
close(con)
info.blog <- paste0("File size: ", fileSize.blog, " Lines in File: ", fileNoOfLines.blog, " Words in file: ", fileWords.blog)

# News text file
con <- file("00ProjectData/Training/final/en_US/en_US.news.txt.")
linesInFile.news <- readLines(con,encoding="UTF-8",skipNul = TRUE)
fileSize.news <- format(object.size(linesInFile.news),units = "MB")
fileNoOfLines.news <- length(linesInFile.news)
fileWords.news <- sum(stri_count_words(linesInFile.news))
close(con)
info.news <- paste0("File size: ", fileSize.news, " Lines in File: ", fileNoOfLines.news, " Words in file: ", fileWords.news)

# Twitter text file
con <- file("00ProjectData/Training/final/en_US/en_US.twitter.txt", "r")
linesInFile.twitter <- readLines(con,encoding="UTF-8",skipNul = TRUE)
fileSize.twitter <- format(object.size(linesInFile.twitter),units = "Mb")
fileNoOfLines.twitter <- length(linesInFile.twitter)
fileWords.twitter <- sum(stri_count_words(linesInFile.twitter))
close(con)
info.twitter <- paste0("File size: ", fileSize.twitter, " Lines in File: ", fileNoOfLines.twitter, " Words in file: ", fileWords.twitter)

# Sampling from data
SampleData <- sapply(list(sample(linesInFile.blog,length(linesInFile.blog)*0.001),
                          sample(linesInFile.news,length(linesInFile.news)*0.25),
                          sample(linesInFile.twitter,length(linesInFile.twitter)*0.25)),
                     rbind)



# Summary of data
DataSummary <- data.frame(Document = c("blog","news","twitter"), 
          size = c(fileSize.blog,fileSize.news,fileSize.twitter),
          NumberOfLines = c(fileNoOfLines.blog,fileNoOfLines.news,fileNoOfLines.twitter),
          NumberOfWords = c(fileWords.blog,fileWords.news,fileWords.twitter),
          Sampled0.1 = c(length(linesInFile.blog)*0.001,
                                     length(linesInFile.news),
                                     length(linesInFile.twitter))  )
DataSummary

# Saving data samples in RDS
saveRDS(SampleData, "SampleDataTinyListBig.rds")

# Cleaning the environment 
rm(list=ls())
```

# Preprocessing 
After cleaning up the environ ment and the dataset samples is stored, the data is then loaded and prepared for the pre-processing which include:
\item Punctuation replacement
\item Converting to lower case
\item Removing numbers
\item Removing stopwords
\item Removing extra white spaces
\item Word-stemming 
\item Converting the document to plain text 

```{r pre processing, eval}
# Reading data
data <- readRDS("SampleDataTinyListBig.rds")

# Creatoing the corpus
cor <- VCorpus(VectorSource(data),readerControl = list(language = "en_US"))

# PunctToSpace <- content_transformer(function(x) gsub("[^[:alnum:][:space:]'`]", "", x))

# Remove punctuation
# cor <- tm_map(cor, PunctToSpace)

# Transform to lower case 
cor <- tm_map(cor, tolower)

# Remove puntcuation
cor<-tm_map(cor, removePunctuation, preserve_intra_word_dashes=TRUE);

# Strip digits
cor <- tm_map(cor, removeNumbers)

# Remove stopwords
cor <- tm_map(cor, removeWords, stopwords("english"))

# remove whitespace
cor <- tm_map(cor, stripWhitespace)

# Stemming the docs
cor <- tm_map(cor, stemDocument,language = "english")

# converting to plain text doc
cor <- tm_map(cor, PlainTextDocument)

```

# N-gram models using tidytext package
So after pre-processing the following exploratory analysis will be done by using tidytext package, to those that are used to working with ' %>% ' from the dplyr package. 
Part of exploratory analysis is to look at the fancy wordcloud plot

```{r Exploratory Analysis}
# Converting the corpus into a data frame. 
dfCor <- tidy(cor) %>% 
        select(text)
rm(cor)
# Creating  a data 
dfCor1 <- dfCor %>% 
        unnest_tokens(output = word, input = text, drop = TRUE, format = "text" ) %>% 
        anti_join(stop_words) %>% 
        count(word, sort = TRUE)

# Taking a look on the workcloud 
wordcloud(word = dfCor1$word, freq = dfCor1$n, min.freq = 1,max.words = 50, 
          random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))

# Uni-gram data preparation and looking at the top 20
unigram <- dfCor %>%
        unnest_tokens(output = word, input = text, drop = TRUE, format = "text") %>%
        count(word, sort = TRUE) %>%
        filter(word != "") %>% 
        arrange(desc(n)) 

# Bigram data preparation and looking at the top 20
bigram <- dfCor %>%
        unnest_tokens(word, text, token = "ngrams", n = 2) %>%
        separate(word, c("word1", "word2"), sep = " ") %>%
        filter(!word1 %in% stop_words$word) %>%
        filter(!word2 %in% stop_words$word) %>%
        unite(word,word1, word2, sep = " ",na.rm = TRUE) %>%
        count(word, sort = TRUE) %>%
        filter(word != "") %>% 
        arrange(desc(n))
# Trigram data preparation and looking at the top 20
trigram <- dfCor %>%
        unnest_tokens(word, text, token = "ngrams", n = 3,drop = T,format = "text",) %>%
        separate(word, c("word1", "word2","word3"), sep = " ") %>%
        filter(!word1 %in% stop_words$word) %>%
        filter(!word2 %in% stop_words$word) %>%
        filter(!word3 %in% stop_words$word) %>%
        unite(word,word1, word2,word3, sep = " ", na.rm = TRUE) %>%
        count(word, sort = TRUE) %>%
        filter(word != "") %>% 
        arrange(desc(n))

saveRDS(unigram, "unigramNoStem.rds")
saveRDS(bigram, "bigramNoStem.rds")
saveRDS(trigram, "trigramNoStem.rds")

```

# Visualisation of N-grams 
Th following plots shows the top 20 uni- bi-, tri- grams: 

```{r plot grams}
p1 <- ggplot(unigram[1:20,]) + 
        geom_bar(aes(x= reorder(word, n),y=n), stat = "identity", fill = "#de5833") +
        theme_minimal() +
        coord_flip() +
        labs(title = "Top 20 unigrams",
             subtitle = "using Tidytext in R",
             caption = "Data Source:")
p1
        
p2 <- ggplot(bigram[1:20,]) + 
        geom_bar(aes(x= reorder(word, n),y=n), stat = "identity", fill = "#de5833") +
        theme_minimal() +
        coord_flip() +
        labs(title = "Top 20 Bigrams",
             subtitle =  "Using Tidytext in R",
             caption = "Data Source: ")
p2

p3 <- ggplot(trigram[1:20,]) + 
        geom_bar(aes(x= reorder(word, n),y=n), stat = "identity", fill = "#de5833") +
        theme_minimal() +
        coord_flip() +
        labs(title = "Top 20 Trigrams",
             subtitle = "using Tidytext in R",
             caption = "Data Source:")
p3
```


# Further analysis    
The next step i to build predictive models using Katz' n-gram Backoff model. Here is the goal to build both accurate and efficient model. Other options would be to effectivize the current tokenization process exploring other packages. 

The shiny app development is aimed to be intuitive and easy to go with few steps and clear output to the user. The idea is that the user would be able to right a few words and the applications should suggest an upcoming word. 

# Predictive model development 

```{r N-Gram Modelling}
options(java.parameters = "-Xmx1024m")

OnegramToken <- function(x){NGramTokenizer(x, Weka_control(min=1, max=1))}
BigramToken <- function(x){NGramTokenizer(x, Weka_control(min=2, max=2))}
TrigramToken <- function(x){NGramTokenizer(x, Weka_control(min=3, max=3))}

# Function for converting document matrix into a data frame
dtmToDf <- function(DocTermMatrix){
  freq <- colSums(DocTermMatrix)
  terms <- data.frame(term = names(freq), frequency = freq, stringsAsFactors = FALSE)
  terms <- arrange(terms, desc(frequency))
  rownames(terms) <- 1:length(freq)
  return(terms)
}

# Creating DocumentTermMatrix One-Gram
dtm <- DocumentTermMatrix(cor,
                          control = list(tokenize = OnegramToken,
                                         wordLengths=c(1,Inf)))
# Creating data Frame 
OneGramDF <- dtmToDf(as.matrix(dtm))
OneGramDF[1:20,]


# Word cloud
wordcloud(word = OneGramDF$term, freq = OneGramDF$frequency, min.freq = 1,max.words = 50, 
          random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))

# Bar plot:
p1 <- ggplot(OneGramDF[1:20,]) + 
        geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
        theme_minimal() +
        coord_flip() +
        labs(title = "Top 20 unigrams",
             subtitle = "using Tidytext in R",
             caption = "Data Source:")
p1

# Two gram 
# Creating DocumentTermMatrix Twi-Gram
dtm <- DocumentTermMatrix(cor,
                          control = list(tokenize = BigramToken,
                                         wordLengths=c(1,Inf)))
# Creating data Frame 
BiGramDF <- dtmToDf(as.matrix(dtm))
BiGramDF[1:20,]


# Word cloud
wordcloud(word = BiGramDF$term, freq = BiGramDF$frequency, min.freq = 1,max.words = 50, 
          random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))

# Bar plot:
p2 <- ggplot(BiGramDF[1:20,]) + 
        geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
        theme_minimal() +
        coord_flip() +
        labs(title = "Top 20 unigrams",
             subtitle = "using Tidytext in R",
             caption = "Data Source:")
p2

# Trigram 
# Creating DocumentTermMatrix Twi-Gram
dtm <- DocumentTermMatrix(cor,
                          control = list(tokenize = TrigramToken,
                                         wordLengths=c(1,Inf)))
# Creating data Frame 
TriGramDF <- dtmToDf(as.matrix(dtm))
TriGramDF[1:20,]


# Word cloud
wordcloud(word = TriGramDF$term, freq = TriGramDF$frequency, min.freq = 1,max.words = 50, 
          random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))

# Bar plot:
p3 <- ggplot(TriGramDF[1:20,]) + 
        geom_bar(aes(x= reorder(term, frequency),y=frequency), stat = "identity", fill = "#de5833") +
        theme_minimal() +
        coord_flip() +
        labs(title = "Top 20 unigrams",
             subtitle = "using Tidytext in R",
             caption = "Data Source:")
p3

```

```{r Conversion to DataTables}
colnames(OneGramDF)[1] <- "LastTerm"
UniGramTable <- data.table(OneGramDF)

BiGramDF1 <- BiGramDF %>%
  separate(term, c("FirstTerms","LastTerm"),sep = " ") 
BiGramTable <- data.table(BiGramDF1)

TriGramDF1 <- TriGramDF %>% 
  separate(term, c("FirstTerms","MidTerm","LastTerm"),sep = " ") %>% 
  unite("FirstTerms",FirstTerms,MidTerm, sep = " ") 
TriGramTable <- data.table(TriGramDF1)


```

```{r N-grams with discounts}
# Source: https://github.com/ThachNgocTran/KatzBackOffModelImplementationInR/blob/master/cal
source("LeftOverProb.R")
source("GramTableExtended.R")
UniGramTable <- GramTableExtended(UniGramTable)

BiGramTable <- GramTableExtended(BiGramTable)
BiGramTable_leftOverProb = BiGramTable[, .(leftoverprob=LeftOverProb(LastTerm, frequency, discount)), by=FirstTerms]

TriGramTable <- GramTableExtended(TriGramTable)
TriGramTable_leftOverProb = TriGramTable[, .(leftoverprob=LeftOverProb(LastTerm, frequency, discount)), by=FirstTerms]
```

```{r Testing}
library(stringr)

source("SeparateTerms.R")
source("getLastTerms.R")
source("getTriGramProb.R")

probFinal <- getTriGramProb("faith during the")

suggest3Gram <- TriGramTable_leftOverProb[which(TriGramTable_leftOverProb$leftoverprob>=probFinal),]
suggest3Gram[1:20,]

suggest2Gram <- BiGramTable_leftOverProb[which(BiGramTable_leftOverProb$leftoverprob>=probFinal),]
suggest2Gram[1:20,]

suggest1Gram <- UniGramTable[which(UniGramTable$discount>=probFinal),]
suggest1Gram[1:20,]
```

```{r Next word}
colnames(BiGramTable)[1:2] <- c("word1","word2")

TriGramTable <- TriGramTable %>% 
  separate(FirstTerms, c("word1","word2"),sep = " ")
colnames(TriGramTable)[3] <- "word3"

source("PredictionFunctions.R")
predictNextWord("be insensitive")
```


