---
title: "Exploratory analysis and Modelling"
author: "Mohamad Barad"
date: "13/2/2022"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This report is about exploring the data in this capsone project, and getting closer into the statistics in the texts in order to be able to build the predition algorithm, in this data set the only the US datasets will be considered. 


```{r packages}
# Loading Packages 
library(tm)
library(stringi)

```

```{r loading data}
# Blog text file
set.seed(3434)
con <- file("final/en_US/en_US.blogs.txt")
linesInFile.blog <- readLines(con, rbinom(800000,200000,0.5))
fileSize.blog <- format(object.size(linesInFile.blog),units = "MB")
fileNoOfLines.blog <- length(linesInFile.blog)
fileWords.blog <- sum(stri_count_words(linesInFile.blog))
close(con)
cat("File size: ", fileSize.blog, " Lines in File: ", fileNoOfLines.blog, " Words in file: ", fileWords.blog)

# # News text file
# con <- file("final/en_US/en_US.news.txt.")
# linesInFile.news <- readLines(con,1000)
# fileSize.news <- format(object.size(linesInFile.news),units = "MB")
# fileNoOfLines.news <- length(linesInFile.news)
# fileWords.news <- sum(stri_count_words(linesInFile.news))
# close(con)
# cat("File size: ", fileSize.news, " Lines in File: ", fileNoOfLines.news, " Words in file: ", fileWords.news)
# 
# # Twitter text file
# con <- file("final/en_US/en_US.twitter.txt", "r") 
# linesInFile.twitter <- readLines(con,1000)
# fileSize.twitter <- format(object.size(linesInFile.twitter),units = "Mb")
# fileNoOfLines.twitter <- length(linesInFile.twitter)
# fileWords.twitter <- sum(stri_count_words(linesInFile.twitter))
# close(con) 
# cat("File size: ", fileSize.twitter, " Lines in File: ", fileNoOfLines.twitter, " Words in file: ", fileWords.twitter)
```

```{r pre processing}
# inpecting one of the documents
head(linesInFile.blog)
# head(linesInFile.news)
# head(linesInFile.twitter)

# Creatoing the corpus
cor <- VCorpus(VectorSource(linesInFile.blog))

# Function that  removes symbols
symbolsRem <- content_transformer(function(x){
        x <- gsub("[`??????]","'",x)
        # x <- gsub("[^a-z']"," ",x)
        # x <- gsub("'{2,}"," '",x)
        # x <- gsub("' "," ",x)
        # x <- gsub(" '"," ",x)
        x <- gsub("^'","",x)
        x <- gsub("'$","",x)
        x <- gsub("â|€|¦|™" ,"",x)
        x <- gsub("[^ ]{1,}@[^ ]{1,}"," ",x)
        x <- gsub("@[^ ]{1,}"," ",x)
        x <- gsub("#[^ ]{1,}"," ",x)
        x <- gsub("[^ ]{1,}://[^ ]{1,}"," ",x)
        x <- gsub("æ|ã|ï"," ",x)
        x
})
# Function that turns punctuation to space
# PunctToSpace <- content_transformer(function(x, pattern){
#        return(gsub(pattern, " ", x))
# })

# Removing symbols
cor <- tm_map(cor, symbolsRem)

# Turning punctuation into space
# cor <- tm_map(cor, PunctToSpace, "-")
# cor <- tm_map(cor, PunctToSpace, ":")
# cor <- tm_map(cor, PunctToSpace, "'")
# cor <- tm_map(cor, PunctToSpace, "*")
# cor <- tm_map(cor, PunctToSpace, " _")


# Remove punctuation
cor <- tm_map(cor, removePunctuation)

# Transform to lower case 
cor <- tm_map(cor, tolower)

# Strip digits
cor <- tm_map(cor, removeNumbers)

# Remove stopwords
cor <- tm_map(cor, removeWords, stopwords("english"))

# remove whitespace
cor <- tm_map(cor, stripWhitespace)

library(SnowballC)
# Stemming the docs
cor <- tm_map(cor, stemDocument)

# converting to plain text doc
cor <- tm_map(cor, PlainTextDocument)

# inspection
writeLines(as.character(cor_stem[[500]]))
# or
cor[[500]]$content

```


```{r Tokens}
library(tidyverse)
library(tidytext)

dfCor <- tidy(cor_stem)

dfCor %>% 
  unnest_tokens(output = word, input = text ) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  head(50)

unigram <- dfCor %>% 
  unnest_tokens(output = word, input = text) %>% 
  count(word, sort = TRUE) %>% 
  slice(1:20) %>% 
  ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  labs(title = "Top unigrams of Medium iOS App Reviews",
       subtitle = "using Tidytext in R",
       caption = "Data Source: itunesr - iTunes App Store")+
  theme(axis.text.x = element_text(angle = 30, vjust = 0.5, hjust=1))
unigram

bigram <- dfCor %>% 
  unnest_tokens(word, text, token = "ngrams", n = 2) %>% 
  separate(word, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  unite(word,word1, word2, sep = " ",na.rm = TRUE) %>% 
  count(word, sort = TRUE) %>% 
  slice(1:20) %>% 
  ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top Bigrams of Medium iOS App Reviews",
       subtitle = "using Tidytext in R",
       caption = "Data Source: itunesr - iTunes App Store")
bigram

trigram <- dfCor %>% 
  unnest_tokens(word, text, token = "ngrams", n = 3) %>% 
  separate(word, c("word1", "word2","word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word3 %in% stop_words$word) %>% 
  unite(word,word1, word2,word3, sep = " ", na.rm = TRUE) %>% 
  count(word, sort = TRUE,) %>% 
  slice(1:20) %>% 
  ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Top Trigrams of Medium iOS App Reviews",
       subtitle = "using Tidytext in R",
       caption = "Data Source: itunesr - iTunes App Store")
trigram
```



```{r document term matrix}
dtm <- TermDocumentMatrix(cor_stem)

library(tidytext)
# dtm_mat <- as.matrix(dtm)
dfCor <- tidy(dtm)
colnames(dfCor) <- c("Word","Doc","Freq")

dfCor_sort <- sort(rowSums(dfCor[,3]), decreasing = TRUE )

dtm_mat.sort <- sort(rowSums(dtm_mat), decreasing =  TRUE)

# dfCor <- data.frame(word = names(dtm_mat.sort), freq = dtm_mat.sort)

library(wordcloud)
wordcloud(word = dfCor$Word, freq = dfCor$Freq, min.freq = 1,max.words = 50, 
          random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))

```

```{r N-grams building}
# creating data frame for gram building
GramData <- data.frame(text = sapply(cor_stem, as.character), stringsAsFactors = FALSE)
library(RWeka)

# Creating Bi, uni and tri grams
UniTok <- NGramTokenizer(GramData,Weka_control(min =1, max =1))
BiTok <- NGramTokenizer(GramData,Weka_control(min =2, max =2))
TriTok <- NGramTokenizer(GramData,Weka_control(min =3, max =3))

# Converting to dataframes 
UniGram <- data.frame(table(UniTok))
BiGram <- data.frame(table(BiTok))
TriGram <- data.frame(table(TriTok))

# Ordering the freq columns and renaming
UniGram <- UniGram[order(UniGram$Freq, decreasing = TRUE),]
colnames(UniGram) <- c("Word","Frequency")

BiGram <- BiGram[order(BiGram$Freq, decreasing = TRUE),]
colnames(BiGram) <- c("Word","Frequency")

TriGram <- TriGram[order(TriGram$Freq, decreasing = TRUE),]
colnames(TriGram) <- c("Word","Frequency")

# Plotting the N-grams
library(ggplot2)
plotUnigram <- ggplot(UniGram[1:10,], aes(x = reorder(Word, Frequency),y=Frequency))+
        geom_bar(stat = "Identity", fill = "red")+
        geom_text(aes(y=Frequency, label = Frequency), vjust= 1) +  
        coord_flip() +
        labs(x = "Word", y = "Frequency", title = "Unigram Frequency")
plotUnigram       



```


